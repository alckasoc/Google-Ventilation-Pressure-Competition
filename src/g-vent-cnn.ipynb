{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import gc\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\n!pip install wandb -qqq\nimport wandb\n# from wandb.keras import WandbCallback\nwandb.login()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-01T20:06:46.033499Z","iopub.execute_input":"2021-10-01T20:06:46.033805Z","iopub.status.idle":"2021-10-01T20:07:35.053532Z","shell.execute_reply.started":"2021-10-01T20:06:46.033732Z","shell.execute_reply":"2021-10-01T20:07:35.052699Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Loading in DataFrames","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-01T20:07:35.055152Z","iopub.execute_input":"2021-10-01T20:07:35.055395Z","iopub.status.idle":"2021-10-01T20:07:46.812629Z","shell.execute_reply.started":"2021-10-01T20:07:35.055362Z","shell.execute_reply":"2021-10-01T20:07:46.811608Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Feature Engineering Function","metadata":{}},{"cell_type":"code","source":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n    \n    #df['u_in_lag']=0\n    #df['u_in_lag2']=0\n    #for i in range(df.shape[0]):\n        #if df['breath_id'][i]==df['breath_id'][i+1]:\n        #    df['u_in_lag'][i+1]=df['u_in'][i]\n        #else:\n        #    df['u_in_lag'][i+1]=0\n        #if df['breath_id'][i]==df['breath_id'][i+2]:\n        #    df['u_in_lag'][i+2]=df['u_in'][i]\n        #else:\n        #    df['u_in_lag'][i+2]=0\n        #if i/10000==round(i/10000):\n        #    print(i)\n    \n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['u_in_lag'] = df['u_in_lag']*df['breath_id_lagsame']\n    df['u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['u_in_lag2'] = df['u_in_lag2']*df['breath_id_lag2same']\n    df['u_out_lag2'] = df['u_out'].shift(2).fillna(0)\n    df['u_out_lag2'] = df['u_out_lag2']*df['breath_id_lag2same']\n    #df['u_in_lag'] = df['u_in'].shift(2).fillna(0)\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['RC'] = df['R']+df['C']\n    df = pd.get_dummies(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-01T20:07:46.817173Z","iopub.execute_input":"2021-10-01T20:07:46.817722Z","iopub.status.idle":"2021-10-01T20:07:46.839842Z","shell.execute_reply.started":"2021-10-01T20:07:46.817682Z","shell.execute_reply":"2021-10-01T20:07:46.838789Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing the Dataset","metadata":{}},{"cell_type":"code","source":"# Feature Engineering.\ntrain_ft = add_features(train)\ntest_ft = add_features(test)\n\n# Filling missing values.\ntrain_ft = train_ft.fillna(0)\ntest_ft = test_ft.fillna(0)\n\n# Getting targets and dropping unimportant features.\ntargets = train_ft[['pressure']].to_numpy().reshape(-1, 80)\ntrain_ft.drop(['pressure','id', 'breath_id','one','count','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same','u_out_lag2'], axis=1, inplace=True)\n\n# Getting Test IDs and dropping unimportant features.\ntest_ids = test[\"id\"]\ntest_ft = test_ft.drop(['id', 'breath_id','one','count','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same','u_out_lag2'], axis=1)\n\n# # Scaling.\n# RS = RobustScaler()\n# train_ft = RS.fit_transform(train_ft)\n# test_ft = RS.transform(test_ft)\n\n# # Reshaping.\n# train_ft = train_ft.reshape(-1, 80, train_ft.shape[-1])\n# test_ft = test_ft.reshape(-1, 80, train_ft.shape[-1])\n# test_ids = np.array(test_ids).reshape(50300, 80)\n\n# Getting u_out values.\n# train_u_out = train.u_out.values.reshape(-1, 80)\n# test_u_out = test.u_out.values.reshape(-1, 80)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T20:22:16.788073Z","iopub.execute_input":"2021-10-01T20:22:16.788653Z","iopub.status.idle":"2021-10-01T20:22:29.696322Z","shell.execute_reply.started":"2021-10-01T20:22:16.788618Z","shell.execute_reply":"2021-10-01T20:22:29.695531Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_ft","metadata":{"execution":{"iopub.status.busy":"2021-10-01T20:22:36.494725Z","iopub.execute_input":"2021-10-01T20:22:36.495589Z","iopub.status.idle":"2021-10-01T20:22:37.334202Z","shell.execute_reply.started":"2021-10-01T20:22:36.495545Z","shell.execute_reply":"2021-10-01T20:22:37.333540Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"RS = RobustScaler()\ntrain_ft = RS.fit_transform(train_ft)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T20:21:18.493921Z","iopub.execute_input":"2021-10-01T20:21:18.494669Z","iopub.status.idle":"2021-10-01T20:21:22.579793Z","shell.execute_reply.started":"2021-10-01T20:21:18.494635Z","shell.execute_reply":"2021-10-01T20:21:22.579072Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"pd.Series(train_ft[:, 2]).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T20:21:47.282208Z","iopub.execute_input":"2021-10-01T20:21:47.283089Z","iopub.status.idle":"2021-10-01T20:21:47.349238Z","shell.execute_reply.started":"2021-10-01T20:21:47.283043Z","shell.execute_reply":"2021-10-01T20:21:47.348448Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Dataset","metadata":{}},{"cell_type":"code","source":"class GeneralDataset(Dataset):\n    def __init__(self, df, mode=\"train\", targets=None, test_ids=None):\n        super(GeneralDataset, self).__init__()\n        assert mode in [\"train\", \"test\"]\n        self.mode = mode\n        self.df = df\n        self.targets = targets\n        self.test_ids = test_ids\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        if self.mode == \"train\":\n            X = torch.FloatTensor(self.df[idx])\n            X = torch.unsqueeze(X, 0)\n            y = torch.FloatTensor(self.targets[idx])\n            return X, y\n        \n        ids = self.test_ids[idx]\n        X = torch.FloatTensor(self.df[idx])\n        X = torch.unsqueeze(X, 0)\n        return ids, X\n    \nclass BiGRUDataset(Dataset):\n    def __init__(self, df, u_out_values, mode=\"train\", targets=None, test_ids=None):\n        super(BiGRUDataset, self).__init__()\n        assert mode in [\"train\", \"test\"]\n        self.df = df\n        self.u_out_values = u_out_values\n        self.mode = mode\n        self.targets = targets\n        self.test_ids = test_ids\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        if self.mode == \"train\":\n            X = torch.FloatTensor(self.df[idx])\n            y = torch.FloatTensor(self.targets[idx])\n            u_out_values = self.u_out_values[idx]\n            return X, y\n        \n        ids = self.test_ids[idx]\n        X = torch.FloatTensor(self.df[idx])\n        return ids, X","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:46:56.111129Z","iopub.execute_input":"2021-10-01T19:46:56.111378Z","iopub.status.idle":"2021-10-01T19:46:56.124533Z","shell.execute_reply.started":"2021-10-01T19:46:56.111347Z","shell.execute_reply":"2021-10-01T19:46:56.122228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model","metadata":{}},{"cell_type":"code","source":"def mish(x):\n    return (x*torch.tanh(F.softplus(x)))\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv_l1 = nn.Conv2d(in_channels=1, out_channels=24, padding=1, stride=1, kernel_size=3)\n        self.conv_l2 = nn.Conv2d(in_channels=24, out_channels=48, padding=0, stride=2, kernel_size=3)\n        # self.linear_l1 = nn.Linear(3744, 4192)\n        self.linear_l1 = nn.Linear(22464, 4192)\n        self.linear_l2 = nn.Linear(4192, 512)\n        self.linear_l3 = nn.Linear(512, 512)\n        self.output = nn.Linear(512, 80)\n\n    def forward(self, x):  # [B, 1, 80, 25]\n        x = F.relu(self.conv_l1(x))  # [B, 24, 80, 25]\n        x = mish(self.conv_l2(x))  # [B, 48, 39, 12]\n        x = x.reshape(x.shape[0], -1)  # [B, 22464]\n        x = mish(self.linear_l1(x))  # [B, 4192]\n        x = F.relu(self.linear_l2(x))  # [B, 512]\n        x = F.relu(self.linear_l3(x))  # [B, 512]\n        x = mish(self.output(x))  # [B, 80]\n        return x\n\nclass BiGRUModel(nn.Module):\n    def __init__(self, n_features, h_features=80):\n        super(BiGRUModel, self).__init__()\n        self.GRU_block1 = nn.GRU(n_features, 512, num_layers=2, bidirectional=True, batch_first=True)\n        self.GRU_block2 = nn.GRU(1024, 256, num_layers=2, bidirectional=True, batch_first=True) \n        self.GRU_block3 = nn.GRU(512, 128, num_layers=2, bidirectional=True, batch_first=True) \n        self.GRU_block4 = nn.GRU(256, 64, num_layers=2, bidirectional=True, batch_first=True)\n        self.GRU_block5 = nn.GRU(128, h_features, num_layers=2, bidirectional=True, batch_first=True) \n#         self.GRU_block6 = nn.GRU(128, 256, num_layers=2, bidirectional=True, batch_first=True) \n#         self.GRU_block7 = nn.GRU(256, 512, num_layers=2, bidirectional=True, batch_first=True) \n#         self.GRU_block8 = nn.GRU(512, , num_layers=2, bidirectional=True, batch_first=True) \n        \n        self.fc1 = nn.Linear(h_features * 2, 32)\n        self.fc2 = nn.Linear(32, 1)\n\n    def forward(self, x):  # [B, 80, n_features]\n        x, _ = self.GRU_block1(x)  # [B, 80, 1024]\n        x = mish(x)\n        x, _ = self.GRU_block2(x)  # [B, 80, 512]\n        x = mish(x)\n        x, _ = self.GRU_block3(x)  # [B, 80, 256]\n        x = mish(x)\n        x, _ = self.GRU_block4(x)  # [B, 80, 128]\n        x = mish(x)\n        x, _ = self.GRU_block5(x)  # [B, 80, 160]\n        x = mish(x)\n        \n        x = F.relu(self.fc1(x))  # [B, 80, 32]\n        x = F.relu(self.fc2(x))  # [B, 80, 1]\n        \n        x = torch.squeeze(x, -1)  # [B, 80]\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:46:56.126008Z","iopub.execute_input":"2021-10-01T19:46:56.12648Z","iopub.status.idle":"2021-10-01T19:46:56.146577Z","shell.execute_reply.started":"2021-10-01T19:46:56.126408Z","shell.execute_reply":"2021-10-01T19:46:56.145896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Script","metadata":{}},{"cell_type":"code","source":"# run.finish()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:46:56.147651Z","iopub.execute_input":"2021-10-01T19:46:56.148022Z","iopub.status.idle":"2021-10-01T19:46:56.158689Z","shell.execute_reply.started":"2021-10-01T19:46:56.147987Z","shell.execute_reply":"2021-10-01T19:46:56.157966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters for CNN","metadata":{}},{"cell_type":"code","source":"ckpt_dir = \".\"\nmodel_name = \"CNN\"\nproject_name = f\"G-Vent-{model_name}\"\nn_features = train_ft.shape[-1]\nn_splits = 5\nn_epochs = 10\ninit_lr = 1e-3\nb_size = 512\nnum_workers = 16","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:46:56.160131Z","iopub.execute_input":"2021-10-01T19:46:56.160442Z","iopub.status.idle":"2021-10-01T19:46:56.168145Z","shell.execute_reply.started":"2021-10-01T19:46:56.160406Z","shell.execute_reply":"2021-10-01T19:46:56.16743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN Training","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=n_splits, shuffle=True, random_state=2021)\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train_ft, targets)):\n    \n    CHECKPOINT = '{}/{}_{}.pth'.format(ckpt_dir, model_name, fold)\n    \n    run = wandb.init(project=project_name, name=f\"fold{fold}\")\n    \n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    X_train, X_valid = train_ft[train_idx], train_ft[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n    \n    train_ds = GeneralDataset(X_train, mode=\"train\", targets=y_train)\n    valid_ds = GeneralDataset(X_valid, mode=\"train\", targets=y_valid)\n    \n    train_loader = DataLoader(train_ds, batch_size=b_size, sampler=RandomSampler(train_ds), num_workers=num_workers)\n    valid_loader = DataLoader(valid_ds, batch_size=b_size, num_workers=num_workers)\n    \n    model = CNN()\n    \n    criterion = nn.L1Loss()\n    scaler = torch.cuda.amp.GradScaler()\n    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs - 1)\n    \n    val_loss_min = np.Inf\n    \n    for idx, epoch in enumerate(range(1, n_epochs + 1)):\n        scheduler.step()\n        model.to(device).train()\n        train_loss = []\n\n        print('Epoch: {:02d}/{:02d}'.format(epoch, n_epochs))\n        print(\"TRAIN\")\n\n        loop = tqdm(train_loader)\n        for X, y in loop:\n            X = X.to(device).float()\n            y = y.to(device).float()\n\n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                output = model(X)\n                loss = criterion(output, y)\n            \n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss.append(loss.item())\n            loop.set_description('current_loss: {:.5f}'.format(loss.item()))\n            loop.set_postfix(loss=np.mean(train_loss))\n        train_loss = np.mean(train_loss)\n    \n        model.eval()\n\n        val_loss = []\n\n        print(\"VAL\")\n        loop = tqdm(valid_loader)\n        for X, y in loop:\n            X = X.to(device).float()\n            y = y.to(device).float()\n\n            with torch.cuda.amp.autocast(), torch.no_grad():\n                outputs = model(X)\n                loss = criterion(outputs.float(), y)\n\n            val_loss.append(loss.item())\n            loop.set_description('current_loss: {:.5f}'.format(loss.item()))\n            loop.set_postfix(loss=np.mean(val_loss))\n        val_loss = np.mean(val_loss)\n        \n        wandb.log({\"epoch\": epoch, \n                \"loss\": train_loss, \n                \"val_loss\": val_loss,\n                })\n        \n        if val_loss < val_loss_min:\n            print('Valid loss improved from {:.5f} to {:.5f} saving model to {}'.format(val_loss_min, val_loss, CHECKPOINT))\n            val_loss_min = val_loss\n            torch.save(model.state_dict(), CHECKPOINT)\n            artifact = wandb.Artifact(model_name, type='model')\n            artifact.add_file(CHECKPOINT, name=f\"fold{fold}_epoch{epoch}.pt\")\n            run.log_artifact(artifact)\n        \n    del model, optimizer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    run.finish()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T04:39:01.517668Z","iopub.execute_input":"2021-10-01T04:39:01.518128Z","iopub.status.idle":"2021-10-01T04:59:21.880368Z","shell.execute_reply.started":"2021-10-01T04:39:01.518092Z","shell.execute_reply":"2021-10-01T04:59:21.879449Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters for BiGRU","metadata":{}},{"cell_type":"code","source":"ckpt_dir = \".\"\nmodel_name = \"BiGRU\"\nproject_name = f\"G-Vent-{model_name}\"\nn_features = train_ft.shape[-1]\nn_splits = 5\nn_epochs = 10\ninit_lr = 1e-3\nb_size = 512\nnum_workers = 16","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:46:56.169195Z","iopub.execute_input":"2021-10-01T19:46:56.169674Z","iopub.status.idle":"2021-10-01T19:46:56.179376Z","shell.execute_reply.started":"2021-10-01T19:46:56.169635Z","shell.execute_reply":"2021-10-01T19:46:56.178758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BiGRU Training","metadata":{}},{"cell_type":"code","source":"train_ft.shape, targets.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:46:56.180623Z","iopub.execute_input":"2021-10-01T19:46:56.180995Z","iopub.status.idle":"2021-10-01T19:46:56.191579Z","shell.execute_reply.started":"2021-10-01T19:46:56.180961Z","shell.execute_reply":"2021-10-01T19:46:56.190894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.finish()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:49:46.835326Z","iopub.execute_input":"2021-10-01T19:49:46.836094Z","iopub.status.idle":"2021-10-01T19:49:50.52111Z","shell.execute_reply.started":"2021-10-01T19:49:46.836056Z","shell.execute_reply":"2021-10-01T19:49:50.520468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GVent_MAE(y, preds, u_out):\n    \"\"\"\n    Metric for the problem\n    \"\"\"\n    w = 1 - u_out\n    \n    assert y.shape == preds.shape and w.shape == y.shape, (y.shape, preds.shape, w.shape)\n    \n    mae = w * torch.abs(y - preds)\n    mae = mae.sum() / w.sum()\n    \n    return mae","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=n_splits, shuffle=True, random_state=2021)\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train_ft, targets)):\n    \n    CHECKPOINT = '{}/{}_{}.pth'.format(ckpt_dir, model_name, fold)\n    \n    run = wandb.init(project=project_name, name=f\"fold{fold}\")\n    \n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    X_train, X_valid = train_ft[train_idx], train_ft[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n    \n    train_ds = BiGRUDataset(X_train, mode=\"train\", targets=y_train)\n    valid_ds = BiGRUDataset(X_valid, mode=\"train\", targets=y_valid)\n    \n    train_loader = DataLoader(train_ds, batch_size=b_size, sampler=RandomSampler(train_ds), num_workers=num_workers)\n    valid_loader = DataLoader(valid_ds, batch_size=b_size, num_workers=num_workers)\n    \n    model = BiGRUModel(n_features=train_ft.shape[-1])\n    \n    criterion = nn.L1Loss()\n    scaler = torch.cuda.amp.GradScaler()\n    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 1e-5, n_epochs - 1)\n    \n    val_loss_min = np.Inf\n    \n    for idx, epoch in enumerate(range(1, n_epochs + 1)):\n        scheduler.step()\n        model.to(device).train()\n        train_loss = []\n\n        print('Epoch: {:02d}/{:02d}'.format(epoch, n_epochs))\n        print(\"TRAIN\")\n\n        loop = tqdm(train_loader)\n        for X, y in loop:\n            X = X.to(device).float()\n            y = y.to(device).float()\n            \n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                output = model(X)\n                loss = criterion(output, y)\n            \n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss.append(loss.item())\n            loop.set_description('current_loss: {:.5f}'.format(loss.item()))\n            loop.set_postfix(loss=np.mean(train_loss))\n        train_loss = np.mean(train_loss)\n    \n        model.eval()\n\n        val_loss = []\n\n        print(\"VAL\")\n        loop = tqdm(valid_loader)\n        for X, y in loop:\n            X = X.to(device).float()\n            y = y.to(device).float()\n\n            with torch.cuda.amp.autocast(), torch.no_grad():\n                outputs = model(X)\n                loss = criterion(outputs.float(), y)\n\n            val_loss.append(loss.item())\n            loop.set_description('current_loss: {:.5f}'.format(loss.item()))\n            loop.set_postfix(loss=np.mean(val_loss))\n        val_loss = np.mean(val_loss)\n        \n        wandb.log({\"epoch\": epoch, \n                \"loss\": train_loss, \n                \"val_loss\": val_loss,\n                })\n        \n        if val_loss < val_loss_min:\n            print('Valid loss improved from {:.5f} to {:.5f} saving model to {}'.format(val_loss_min, val_loss, CHECKPOINT))\n            val_loss_min = val_loss\n            torch.save(model.state_dict(), CHECKPOINT)\n            artifact = wandb.Artifact(model_name, type='model')\n            artifact.add_file(CHECKPOINT, name=f\"fold{fold}_epoch{epoch}.pt\")\n            run.log_artifact(artifact)\n        \n    del model, optimizer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    run.finish()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:47:02.373886Z","iopub.execute_input":"2021-10-01T19:47:02.37421Z","iopub.status.idle":"2021-10-01T19:49:41.204159Z","shell.execute_reply.started":"2021-10-01T19:47:02.374179Z","shell.execute_reply":"2021-10-01T19:49:41.202233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test_ft.shape, len(test_ids)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:43:28.199532Z","iopub.execute_input":"2021-10-01T05:43:28.19999Z","iopub.status.idle":"2021-10-01T05:43:28.21075Z","shell.execute_reply.started":"2021-10-01T05:43:28.199956Z","shell.execute_reply":"2021-10-01T05:43:28.209935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = GeneralDataset(test_ft, mode=\"test\", test_ids=test_ids)\ntest_loader = DataLoader(test_ds, batch_size=b_size, num_workers=0)\n\nmodels = {}\nmodel_paths = [r\"../input/gventcnn-weights/fold0_epoch10\",\n               r\"../input/gventcnn-weights/fold1_epoch10\",\n               r\"../input/gventcnn-weights/fold2_epoch10\",\n               r\"../input/gventcnn-weights/fold3_epoch10\",\n               r\"../input/gventcnn-weights/fold4_epoch10\",]\n\nfor fold, path in enumerate(model_paths):\n    model = CNN()\n    model.load_state_dict(torch.load(path))\n    models[fold] = model\n\nens_preds = []\nfor fold, model in models.items():\n    model.eval().to(device)\n    \n    ids_list = []\n    preds = []\n    \n    loop = tqdm(test_loader)\n    for ids, X in loop:\n        X = X.to(device).float()\n        ids_list.append(ids)\n\n        with torch.no_grad():\n            outputs = model(X)\n            preds.append(outputs.data.cpu().numpy())\n    \n    preds = np.concatenate(preds).flatten()\n    ens_preds.append(preds)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:48:59.715521Z","iopub.execute_input":"2021-10-01T05:48:59.715779Z","iopub.status.idle":"2021-10-01T05:49:23.537327Z","shell.execute_reply.started":"2021-10-01T05:48:59.715751Z","shell.execute_reply":"2021-10-01T05:49:23.536609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": np.concatenate(ids_list).flatten(),\n    \"pressure\": np.mean(ens_preds, axis=0)\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:51:33.828953Z","iopub.execute_input":"2021-10-01T05:51:33.829238Z","iopub.status.idle":"2021-10-01T05:51:43.954392Z","shell.execute_reply.started":"2021-10-01T05:51:33.829193Z","shell.execute_reply":"2021-10-01T05:51:43.953574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}