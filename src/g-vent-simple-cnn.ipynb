{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-30T21:31:42.764879Z","iopub.execute_input":"2021-09-30T21:31:42.765194Z","iopub.status.idle":"2021-09-30T21:31:42.867118Z","shell.execute_reply.started":"2021-09-30T21:31:42.765115Z","shell.execute_reply":"2021-09-30T21:31:42.866287Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ndef mish(x):\n    return (x*torch.tanh(F.softplus(x)))\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv_l1 = nn.Conv2d(in_channels=1, out_channels=24, padding=1, stride=1, kernel_size=3)\n        self.conv_l2 = nn.Conv2d(in_channels=24, out_channels=48, padding=0, stride=2, kernel_size=3)\n        self.linear_l1 = nn.Linear(3744, 4192)\n        self.linear_l2 = nn.Linear(4192, 512)\n        self.linear_l3 = nn.Linear(512, 512)\n        self.output = nn.Linear(512, 80)\n\n\n    def forward(self, x):\n        x = F.relu(self.conv_l1(x))\n        x = mish(self.conv_l2(x))\n        x = x.reshape(x.shape[0], -1)\n        x = mish(self.linear_l1(x))\n        x = F.relu(self.linear_l2(x))\n        x = F.relu(self.linear_l3(x))\n        x = mish(self.output(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:31:43.042094Z","iopub.execute_input":"2021-09-30T21:31:43.042310Z","iopub.status.idle":"2021-09-30T21:31:47.384190Z","shell.execute_reply.started":"2021-09-30T21:31:43.042287Z","shell.execute_reply":"2021-09-30T21:31:47.383481Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nclass TrainDset(Dataset):\n    def __init__(self, paths):\n        self.paths = paths\n\n\n    def __len__(self):\n        return len(self.paths)\n\n\n    def __getitem__(self, idx):\n        df = pd.read_csv(self.paths[idx], index_col='id')\n        y = df.pressure.values\n        tensor = df.drop(columns=['breath_id', 'pressure']).values\n        tensor = tensor.reshape(1, tensor.shape[0], tensor.shape[1])\n        return {\"tensor\": torch.tensor(tensor), \"targets\": y, \"u_out\": df.u_out.values}","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:31:47.385888Z","iopub.execute_input":"2021-09-30T21:31:47.386191Z","iopub.status.idle":"2021-09-30T21:31:47.393925Z","shell.execute_reply.started":"2021-09-30T21:31:47.386155Z","shell.execute_reply":"2021-09-30T21:31:47.393072Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\n\ndef split_by_breath_id():\n    modes = (\"train\", \"test\")\n    for mode in modes:\n        os.makedirs(f\"./{mode}/\", exist_ok=True)\n    train_paths = []\n    test_paths = []\n    \n    for split in modes:\n        df = pd.read_csv(f\"../input/ventilator-pressure-prediction/{split}.csv\")\n\n        unique_breath_ids = df.breath_id.unique()\n\n        for breath_id in tqdm(unique_breath_ids):\n            id_df =  df[df.breath_id == breath_id]\n            p = f\"./{split}/{breath_id}.csv\"\n            id_df.to_csv(p, index=False)\n            if split == \"train\": train_paths.append(p)\n            if split == \"test\": test_paths.append(p)\n    return train_paths, test_paths","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:31:47.395446Z","iopub.execute_input":"2021-09-30T21:31:47.395795Z","iopub.status.idle":"2021-09-30T21:31:47.406142Z","shell.execute_reply.started":"2021-09-30T21:31:47.395762Z","shell.execute_reply":"2021-09-30T21:31:47.405529Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_paths, test_paths = split_by_breath_id()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:31:50.285397Z","iopub.execute_input":"2021-09-30T21:31:50.285945Z","iopub.status.idle":"2021-09-30T21:45:57.956871Z","shell.execute_reply.started":"2021-09-30T21:31:50.285909Z","shell.execute_reply":"2021-09-30T21:45:57.956173Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(train_paths), len(test_paths)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:45:57.958491Z","iopub.execute_input":"2021-09-30T21:45:57.958905Z","iopub.status.idle":"2021-09-30T21:45:57.967277Z","shell.execute_reply.started":"2021-09-30T21:45:57.958869Z","shell.execute_reply":"2021-09-30T21:45:57.966355Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_ds = TrainDset(train_paths)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:45:57.968637Z","iopub.execute_input":"2021-09-30T21:45:57.968913Z","iopub.status.idle":"2021-09-30T21:45:57.978662Z","shell.execute_reply.started":"2021-09-30T21:45:57.968880Z","shell.execute_reply":"2021-09-30T21:45:57.977857Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_ds[0][\"targets\"].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:45:57.980620Z","iopub.execute_input":"2021-09-30T21:45:57.981010Z","iopub.status.idle":"2021-09-30T21:45:58.013213Z","shell.execute_reply.started":"2021-09-30T21:45:57.980974Z","shell.execute_reply":"2021-09-30T21:45:58.012419Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_paths_, val_paths = train_test_split(train_paths, train_size=0.85)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:45:58.014517Z","iopub.execute_input":"2021-09-30T21:45:58.014780Z","iopub.status.idle":"2021-09-30T21:45:58.871205Z","shell.execute_reply.started":"2021-09-30T21:45:58.014749Z","shell.execute_reply":"2021-09-30T21:45:58.870460Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.sampler import SequentialSampler, RandomSampler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_workers = 16\nb_size = 512\ninit_lr = 1e-3\nn_epochs = 300\n\nX_train_ds = TrainDset(train_paths_)\nX_valid_ds = TrainDset(val_paths)\n\nX_train_loader = DataLoader(X_train_ds, batch_size=b_size, sampler=RandomSampler(X_train_ds), num_workers=num_workers)\nX_valid_loader = DataLoader(X_valid_ds, batch_size=b_size, num_workers=num_workers)\n    \nmodel = CNN()\n    \ncriterion = nn.L1Loss()\nscaler = torch.cuda.amp.GradScaler()\noptimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs - 1)\n    \nval_loss_min = np.Inf\n    \nfor idx, epoch in enumerate(range(1, n_epochs + 1)):\n    scheduler.step()\n    model.to(device).train()\n    train_loss = []\n\n    print('Epoch: {:02d}/{:02d}'.format(epoch, n_epochs))\n    print(\"TRAIN\")\n\n    loop = tqdm(X_train_loader)\n    for batch in loop:\n        X = batch[\"tensor\"].to(device).float()\n        y = batch[\"targets\"].to(device).float()\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            output = model(X)\n            loss = criterion(output, y)\n            \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss.append(loss.item())\n        loop.set_description('current_loss: {:.5f} | LR: {:.5f}'.format(loss.item(), optimizer.param_groups[0]['lr']))\n        loop.set_postfix(loss=np.mean(train_loss))\n    train_loss = np.mean(train_loss)\n    \n    model.eval()\n\n    val_loss = []\n\n    print(\"VAL\")\n    loop = tqdm(X_valid_loader)\n    for batch in loop:\n        X = batch[\"tensor\"].to(device).float()\n        y = batch[\"targets\"].to(device).float()\n\n        # with torch.cuda.amp.autocast():\n        with torch.cuda.amp.autocast(), torch.no_grad():\n            outputs = model(X)\n            loss = criterion(outputs.float(), y)\n\n        val_loss.append(loss.item())\n        loop.set_description('current_loss: {:.5f}'.format(loss.item()))\n        loop.set_postfix(loss=np.mean(val_loss))\n    val_loss = np.mean(val_loss)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T23:13:12.988439Z","iopub.execute_input":"2021-09-29T23:13:12.988716Z","iopub.status.idle":"2021-09-29T23:43:24.767936Z","shell.execute_reply.started":"2021-09-29T23:13:12.988663Z","shell.execute_reply":"2021-09-29T23:43:24.764799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2021-09-29T23:44:35.631607Z","iopub.execute_input":"2021-09-29T23:44:35.632358Z","iopub.status.idle":"2021-09-29T23:44:35.928674Z","shell.execute_reply.started":"2021-09-29T23:44:35.632318Z","shell.execute_reply":"2021-09-29T23:44:35.92785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"class TestDset(Dataset):\n    def __init__(self, paths):\n        self.paths = paths\n\n\n    def __len__(self):\n        return len(self.paths)\n\n\n    def __getitem__(self, idx):\n        df = pd.read_csv(self.paths[idx], index_col='id')\n        tensor = df.drop(columns=['breath_id']).values\n        tensor = tensor.reshape(1, tensor.shape[0], tensor.shape[1])\n        return {\"tensor\": torch.tensor(tensor), \n                \"ids\": np.array(df.index)}","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:54:17.590071Z","iopub.execute_input":"2021-09-30T21:54:17.590338Z","iopub.status.idle":"2021-09-30T21:54:17.597116Z","shell.execute_reply.started":"2021-09-30T21:54:17.590310Z","shell.execute_reply":"2021-09-30T21:54:17.596325Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test_ds = TestDset(test_paths)\ntest_loader = DataLoader(test_ds, batch_size=512, num_workers=16)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CNN()\nmodel.load_state_dict(torch.load(r\"../input/gventcnnsimple/model_weights.pth\"))\nmodel.eval().to(device)\n\npreds, ids_list = [], []\nloop = tqdm(test_loader)\nfor batch in loop:\n    X = batch[\"tensor\"].to(device).float()\n    ids = batch[\"ids\"]\n    ids_list.append(ids)\n\n    with torch.cuda.amp.autocast(), torch.no_grad():\n        outputs = model(X)\n        preds.append(outputs.data.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:54:44.960774Z","iopub.execute_input":"2021-09-30T21:54:44.961709Z","iopub.status.idle":"2021-09-30T21:56:39.020711Z","shell.execute_reply.started":"2021-09-30T21:54:44.961668Z","shell.execute_reply":"2021-09-30T21:56:39.019915Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"np.concatenate(preds).flatten().shape","metadata":{"execution":{"iopub.status.busy":"2021-09-30T22:01:24.446783Z","iopub.execute_input":"2021-09-30T22:01:24.447031Z","iopub.status.idle":"2021-09-30T22:01:24.465847Z","shell.execute_reply.started":"2021-09-30T22:01:24.447005Z","shell.execute_reply":"2021-09-30T22:01:24.465157Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"np.concatenate(ids_list).flatten().shape","metadata":{"execution":{"iopub.status.busy":"2021-09-30T22:01:27.149189Z","iopub.execute_input":"2021-09-30T22:01:27.149442Z","iopub.status.idle":"2021-09-30T22:01:27.172926Z","shell.execute_reply.started":"2021-09-30T22:01:27.149415Z","shell.execute_reply":"2021-09-30T22:01:27.172075Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": np.concatenate(ids_list).flatten(),\n    \"pressure\": np.concatenate(preds).flatten()\n})\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T22:03:01.975958Z","iopub.execute_input":"2021-09-30T22:03:01.976483Z","iopub.status.idle":"2021-09-30T22:03:12.488158Z","shell.execute_reply.started":"2021-09-30T22:03:01.976445Z","shell.execute_reply":"2021-09-30T22:03:12.487297Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}